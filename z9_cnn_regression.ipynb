{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Shape Dimensions: Z9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_load import ShapeDimensionDataset\n",
    "import regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 150\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "omit test for the time being ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['h', 'tw', 'ba', 'ta', 'ra', 'bf', 'tf', 'rf', 'blf', 'tlf', 'rlf']\n",
    "target_cols = [s + \"_scaled\" for s in parameters] \n",
    "\n",
    "flip_transform = regression.flip_transform(IMAGE_SIZE)\n",
    "no_transform = regression.no_transform(IMAGE_SIZE)\n",
    "\n",
    "datasets = {\n",
    "    'train': ShapeDimensionDataset(\"dataset/Z9_train.csv\", \n",
    "                                   \"dataset/train/Z9/\",\n",
    "                                   transform=flip_transform, \n",
    "                                   target_cols=target_cols),\n",
    "    'val': ShapeDimensionDataset(\"dataset/Z9_val.csv\", \n",
    "                                 \"dataset/val/Z9/\",\n",
    "                                 transform=flip_transform, \n",
    "                                 target_cols=target_cols),    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': DataLoader(datasets['train'], \n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   shuffle=True, \n",
    "                                   num_workers=6),\n",
    "               'val': DataLoader(datasets['val'], \n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False, \n",
    "                                 num_workers=6),                                       \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show something\n",
    "\n",
    "Convert PIL grayscale image (type=L) to np array (axes are still flipped):\n",
    "\n",
    "`img = np.frombuffer(img_.tobytes(), dtype=np.uint8).reshape(224,224)`\n",
    "\n",
    "Convert torch.Tensor to np array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel value range: 0.0 1.0\n",
      "shape: (150, 150)\n",
      "dtype: float32\n",
      "[[0.86789816]\n",
      " [0.36512457]\n",
      " [0.34533224]\n",
      " [0.19744704]\n",
      " [0.10438685]\n",
      " [0.26740996]\n",
      " [0.26925324]\n",
      " [0.45775369]\n",
      " [0.07554774]\n",
      " [0.48918266]\n",
      " [0.16400704]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets['train']\n",
    "\n",
    "img_, y = ds[120]  \n",
    "# TODO: how to rotate ???\n",
    "img = img_[0].numpy() # shape(1,224,224) -> shape(224,224)\n",
    "img = img.squeeze()\n",
    "print(\"pixel value range:\", np.min(img), np.max(img))\n",
    "# TODO: these are grayscale (B/W) pictures. One depth dimension is enough.\n",
    "print(\"shape:\", img.shape)\n",
    "print(\"dtype:\", img.dtype)\n",
    "print(y)\n",
    "plt.imshow(img, cmap='gray');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, log=False):\n",
    "    \"\"\"history plot, return image\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history['train'], \"o-\", label=\"train\")\n",
    "    ax.plot(history['val'], \"o-\", label=\"val\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    if log:\n",
    "        ax.semilogy()\n",
    "    ax.grid()\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4624, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "model = regression.Net1(len(parameters), \n",
    "                        image_size=IMAGE_SIZE, \n",
    "                        param_names=parameters, \n",
    "                        description=\"Z9\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "print(model)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train Loss: 0.022407 "
     ]
    }
   ],
   "source": [
    "if 1 == 1:\n",
    "    hist = regression.train_regression(model, dataloaders, criterion, optimizer, num_epochs=10)\n",
    "    plot_history(hist, log=True)\n",
    "    model.save_checkpoint(\"models/Z9_flip_50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, great! It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"history_Z9_flip_50.json\", \"w\") as fp:\n",
    "    json.dump(hist, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sections import VERTEX_FUNCTIONS\n",
    "\n",
    "func, params = VERTEX_FUNCTIONS['z9']\n",
    "\n",
    "def imshow(tensor):\n",
    "    \"\"\"show first image in a tensor\"\"\"\n",
    "    img = tensor[0].numpy() # shape(1,224,224) -> shape(224,224)\n",
    "    img = img.squeeze()\n",
    "    plt.imshow(img, cmap='gray');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = regression.Net1.from_checkpoint(\"models/Z9_flip_50.pt\")\n",
    "\n",
    "# scaler\n",
    "with open(\"dataset/Z9_scaler.pkl\", \"rb\") as fp:\n",
    "    scaler = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 310\n",
    "area = 200\n",
    "\n",
    "# get data point\n",
    "ds = datasets['val']\n",
    "image, target = ds[i]\n",
    "# show image and target\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.reshape(1, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "print(img.max())\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model.forward(img)\n",
    "    outputs = outputs.numpy()\n",
    "    \n",
    "# print raw target / output\n",
    "print(params)\n",
    "\n",
    "print(\"target (scaled): \", target.squeeze())\n",
    "print(\"output (scaled): \", outputs.squeeze())\n",
    "\n",
    "# unscale both, multiply with reference length (sqrt(A))\n",
    "target_dim = np.sqrt(area) * scaler.inverse_transform(target.T)\n",
    "target_dim = target_dim.squeeze()\n",
    "output_dim = np.sqrt(area) * scaler.inverse_transform(outputs)\n",
    "output_dim = output_dim.squeeze()\n",
    "print(\"target (unscaled): \", target_dim)\n",
    "print(\"output (unscaled): \", output_dim)\n",
    "\n",
    "# generate vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "df = pd.DataFrame([target_dim, output_dim], index=['true', 'predicted'], \n",
    "                  columns=params)\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# generate polygons\n",
    "true_o, true_i = func(*target_dim.squeeze())\n",
    "pred_o, pred_i = func(*output_dim.squeeze())\n",
    "\n",
    "\n",
    "for outer in true_o:\n",
    "    mppoly = plt.Polygon(outer, ec=\"k\", ls='--', fill=False, linewidth=1)\n",
    "    ax.add_patch(mppoly)\n",
    "for inner in true_i:\n",
    "    mppoly = plt.Polygon(inner, ec=\"k\", ls='--', fill=False, linewidth=1)\n",
    "    ax.add_patch(mppoly)\n",
    "    \n",
    "for outer in pred_o:\n",
    "    mppoly = plt.Polygon(outer, ec=\"r\", fill=False, linewidth=1)\n",
    "    ax.add_patch(mppoly)\n",
    "for inner in pred_i:\n",
    "    mppoly = plt.Polygon(inner, ec=\"r\", fill=False, linewidth=1)\n",
    "    ax.add_patch(mppoly)\n",
    "\n",
    "ax.autoscale(tight=False)\n",
    "ax.set_aspect(\"equal\")\n",
    "#ax.axis(\"off\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate: Compute IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Loop: Input polygon vertices, create image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
